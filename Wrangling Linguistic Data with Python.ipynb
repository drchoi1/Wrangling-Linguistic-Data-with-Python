{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Wrangling Linguistic Data with Python \n",
    "This workshop will introduce you to the programming language Python and walk you through a typical workflow for converting raw text into an annotated linguistic dataset.  We will cover various computational tasks, including reading in raw text files, segmenting text into sentences and tokens, and annotating tokens for various levels of metadata. We will explore the types of linguistic annotation available in the NLP package SpaCy, such as part-of-speech, lemma, and syntactic function. After annotating texts, we will cover techniques for searching and filtering data and use regular expressions to look for word patterns. This workshop is designed to be accessible to both those who are new to programming as well as those who have experience programming.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Practicing with Objects and Functions\n",
    "We will look at three types of objects here: \n",
    "* string - sequence of characters, marked by single (`'`) or double (`\"`) quotes\n",
    "* integer - a single interger (ex. 1, 2, 3, 4, or 5)\n",
    "* list - group of several objects together, marked by square brackets `[]`\n",
    "\n",
    "In order to reference these objects within our code, we assign them to a variable, using equals sign (=) and then start using it. In order to manipulate and utlizes these objects, we apply functions and methods to them. In essence functions and methods transform something into something else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"This is a string\" # let's assign this string to a variable called \"a\"\n",
    "b = 4 # let's assign this integer to a variable called \"b\"\n",
    "c = [a, b] # let's assign this list of objects to a variable called \"c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a) # use the print function to see what variable \"a\" is\n",
    "print(b) # use the print function to see what variable \"b\" is\n",
    "print(c) # use the print function to see what variable \"c\" is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c) # use the type function to see what type of variable \"a\" is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red>Your turn! (4 minutes) </font> \n",
    "<font color=red> **Beginner:** Create new variables of different types, print them, and verify their types using the `print` and `type` functions. Create a list. Play around with numbers with decimals. What is that variable type called?</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark red> **Intermediate:** Play around with the function `len()` on lists and strings. What does this function do? Try adding integers together and strings together with `+`. What happens? Can you at integers and strings together? \n",
    "Try using [string methods](http://www.python-ds.com/python-3-string-methods), similar to functions Python except they are attached to an object.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark red> **Advanced:** Try using [string methods](http://www.python-ds.com/python-3-string-methods), such as `.upper()` and `.capitalize()`. Methods are similar to functions Python except they are attached to an object. \n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading in Data\n",
    "We first need to read in our file so we can access our data. We will read in a sample file from the Davies Corpus that are readily accessible at https://www.corpusdata.org/formats.asp. All the data files are available in this repository within the Data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Reading in Data\n",
    "First we create an object for the file with the `open` function and then we read it into a variable with the `read` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Data/sp_short_text.txt\")\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red>Your turn! (2 minutes) </font> \n",
    "<font color=red> **Beginner:** What type of variable is `data`? Do you notice anything strange about the text?</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark red> **Intermediate:** Practice reading in another file using the `.readlines()` function. What does this do? What type of object is it? Hint: Don't forget to first open the file.</font> \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading in a data file, it is important look at the data to see if there are any encoding issues or other textual isses that need to be addressed. The Spanish sample text above contains random insertions of the symbol \"@\". This is something we need to remove before we process the data. \n",
    "We will use a new function, `replace` to sustitute the symbols we wish to remove with nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data.replace(\"@\", \"\")\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red>Your turn! (4 minutes) </font> \n",
    "<font color=red> **Beginner**: Did this solve the problem completely? If not, how might you fix it? What other elements might you want to remove? \n",
    "Practice using the `.replace` function on other string objects. </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= dark red> **Advanced:** What is we also wanted to remove the numbers that follow @@? We can use a new function, `re.sub` from the `re` package to use [regular expressions](https://www.w3schools.com/python/python_regex.asp). First we need to import the package using the `import` function. \n",
    "    \n",
    "What regular expression is needed to identify the sequences @@124 @@1124, etc.? </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data_clean = re.sub(\"@ \", \"\", data) #remove all @"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotating Data\n",
    "We will use the Spacy package to create annotated linguistic data. If you have not used this package before, you will first need to install it via the console. You can send a command to the console by prefixing it with \"!\" or alternatively opening up your console and typing the command there. You only need to do this once on a given computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm\n",
    "! python -m spacy download pt_core_news_sm\n",
    "! python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Spacy module and create an object for each language you will use, Spanish, English, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_sp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `nlp_sp` function to create a Spacy Object from the data. Which variable should we use? `data` or `data_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_spacy = nlp_sp(data)\n",
    "data_spacy = nlp_sp(data_clean)\n",
    "type(data_spacy)\n",
    "print(data_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Let's look at how the data_spacy object divides a text into sentences and tokens. But first we need to learn about loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loops\n",
    "If we want to look at every element of a list and perform a repeated task, we use a loop.  \n",
    "\n",
    "The general structure is:\n",
    "```\n",
    "for <element> in <list>:\n",
    "    <do something>\n",
    "    ...\n",
    "```\n",
    "This `for`-loop tells Python to take each element from a `list`, call it `element`, and then do the things that follow to it. Note that the words **`for`** and **`in`** are reserved words, meaning that you should not use them as a variable name. The colon at the end of the line is important part of the syntax and the following indent. It tells Python that everything that comes afterwards is what you want to do with each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = [\"apples\", \"bananas\", \"pears\"]\n",
    "\n",
    "for item in fruits:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_fruits = []\n",
    "for item in fruits:\n",
    "    cap_item = item.capitalize()\n",
    "    cap_fruits.append(cap_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red>Your turn! (4 minutes) </font> \n",
    "<font color=red> **Beginner:** Create a loop that takes each element, deletes the 's' and prints the result. Play around with the `.upper()` and `.swapcase()` methods with the loop to see what they do </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= dark red> **Intermediate:** Make a new list of integers and another empty list. Create a loop that adds 5 to each element, adds the result to the new list. </font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to tokenization. The data_spacy object is like a list in that you can loop over it to access each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the word level\n",
    "for token in data_spacy: \n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate(data_spacy):\n",
    "    print(index, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the sentence level\n",
    "for index, sent in enumerate(data_spacy.sents): # Loops\n",
    "    print(index, sent.text)\n",
    "    \n",
    "#[sent.text for sent in data_spacy.sents] # List Comprehension - ADVANCED\n",
    "\n",
    "len(data_spacy) # number of tokens in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tags\n",
    "Spacy has two types of POS tags. They can be accessed for each token via attributes.\n",
    "* POS: The simple [UPOS](https://universaldependencies.org/docs/u/pos/) part-of-speech tag.\n",
    "* Tag: The detailed part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate(data_spacy): # Loop\n",
    "    print(index, token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering data\n",
    "\"If\" statements can be a useful tool within loops to filter data. The general Python syntax for a simple if statement is. \n",
    "`if condition :\n",
    "    indentedStatementBlock`\n",
    "    \n",
    "`If` statements many utilize a variety of logical conditions, such as:\n",
    "* Equals: a == b\n",
    "* Not Equals: a != b\n",
    "* Less than: a < b\n",
    "* Less than or equal to: a <= b\n",
    "* Greater than: a > b\n",
    "* Greater than or equal to: a >= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is greater than a\n"
     ]
    }
   ],
   "source": [
    "a = 33\n",
    "b = 200\n",
    "if b > a:\n",
    "  print(\"b is greater than a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use an `if` statement within a `for` loop to identify all the verbs within our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in data_spacy:\n",
    "    if token.pos_ == \"VERB\":\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red>Your turn! (4 minutes) </font> \n",
    "<font color=red> **Beginner:** Create an `if` statement, using another logical condition. Try testing both integers and string objects.  </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> **Advanced:** Create a `for` loop with an  `if` statement to identify all the nouns in our text. Can you write a line of code to figure out how many nouns there are total?  </font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in data_spacy: # Loop\n",
    "    print(token.text, token.ent_iob_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in data_spacy.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(token, token.is_stop) for token in data_spacy] # Stop words\n",
    "[(token, token.lemma_) for token in data_spacy] # Lemmas\n",
    "[(token, token.dep_) for token in data_spacy] # dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze syntax\n",
    "[chunk.text for chunk in data_spacy.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp_en(\"This is a sentence.\")\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=red>Your turn! (6 minutes) </font> \n",
    "<font color=red> **Beginner:** Read in a new document. Explore the various tags by looping over the data. Can you view various tags at once? </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a dataframe\n",
    "After exploring the annotation levels available in Spacy, we can create a dataframe that contains the information we are interested in. We will use the `pandas` package to create a dataframe and then output of dataframe into a csv file that can be read into R for statistical analysis.First we will import the `pandas` package using the `import` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Token'] = [token.text for token in data_spacy]\n",
    "df['POS'] = [token.pos_ for token in data_spacy]\n",
    "df['NE'] = [token.ent_iob_ for token in data_spacy]\n",
    "df['Lemma'] = [token.lemma_ for token in data_spacy]\n",
    "df['Tag'] = [token.tag_ for token in data_spacy]\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>Your turn!</font> \n",
    "\n",
    "<font color=red>Create a dataframe with a column for POS, NE, and Lemma</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark red> **Advanced:** Create a dataframe only of the nouns appearing in the text. Have a column for lemma, NE and _is_stop annotations. Can you create a column containing the sentence each noun comes from?</font>  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'Data/SpacyDF.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
