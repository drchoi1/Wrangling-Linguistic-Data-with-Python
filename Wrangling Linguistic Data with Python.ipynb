{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Wrangling Linguistic Data with Python \n",
    "Let's start off with a quick survey to see what everyone's programming background is. https://forms.gle/DvzQnyWsPcuE4jQC6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Practice with Objects and Functions\n",
    "A variable is the name attached to a particular object. We will look at three types of objects here: strings, integers, and lists  To create a variable, you just assign it a value with the equals sign (=) and then start using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"This is a string\" # let's assign this text to a variable called \"a\"\n",
    "b = 4 # let's assign this number to a variable called \"b\"\n",
    "c = [a, b] # let's assign this list of objects to a variable called \"c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string\n",
      "4\n",
      "['This is a string', 4]\n"
     ]
    }
   ],
   "source": [
    "print(a) # use the print function to see what variable \"a\" is\n",
    "print(b) # use the print function to see what variable \"b\" is\n",
    "print(c) # use the print function to see what variable \"c\" is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c) # use the type function to see what type of variable \"a\" is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn!\n",
    "Create new variables of different types, print them, and verify their types using the `print` and `type` functions. Play around with decimal numbers. What is that variable type called?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading in Data\n",
    "We first need to read our file so we can access our data. The type of file your data is in determines what function you should use to read in your data. We will read in several sample files from the Davies Corpus that are readily accessible at https://www.corpusdata.org/formats.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a single plain text file, ending in .txt\n",
    "f = open(\"Data/spanish-sample-text/text.txt\")\n",
    "data = f.read()\n",
    "data[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a directory of files\n",
    "import os\n",
    "\n",
    "#with os.scandir('my_directory/') as entries:\n",
    "#    for entry in entries:\n",
    "#        print(entry.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "Choose either the basic or advanced method to read in the following three files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading in a data file, it is important look at the data read in to see if there are any encoding issues or other textual isses that need to be addressed. The Spanish Sample text above contains random insertions of the symbol \"@\". This is something we need to remove before we process the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'textID\\ttext\\n----\\t----\\n\\n124 Gran convocatoria para el concurso docente que se realiza en la Escuela Normal Con una inmensa convocatoria de docentes , convocada desde la 7.30 de este lunes en el salón de actos de la Escuela Normal Mariano Moreno , se realizó la primera jornada de el concurso para titularización de los cargos . El día comenzó con las palabras de bienvenidas de las autoridades , quienes hablaron de cientos de docentes que presentaron sus documentos que deben ser analizados por las autoridades . Los cargos fueron 138 , pero esa suma se incrementó debido a que muchos realizaron cambio de escuelas , abriendo otras oportunidades . Las jornadas continuaran este martes debido , precisamente , a el enorme número de docentes presentes . Estuvieron , la presidenta de el CGE , Graciela Bar , el profesor Héctor de la Fuente , vocal de presidencia , el secretario general de Agmer Fabián Peccín y la directora Departamental de Escuela , María del Carmen Tourfini de Córdoba . Más allá de'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "data_clean = re.sub(\"@+\", \"\", data)\n",
    "data_clean[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did this solve the problem? If not, how can it be resolve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Processing Data\n",
    "We will use the Spacy package to create annotated linguistic data. If you have not used this package before, you will first need to install it via the console. You send a command to the console by prefixing it with \"!\" or alternatively opening up your console and typing the command there. You only need to do this once on a given computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: pt_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz#egg=pt_core_news_sm==2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n",
      "Requirement already satisfied: es_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.1.0/es_core_news_sm-2.1.0.tar.gz#egg=es_core_news_sm==2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#! conda install -c conda-forge spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "! python -m spacy download pt_core_news_sm\n",
    "! python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Spacy module and create an object for each language you will use, Spanish, English, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_sp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 11986988 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-afc35f8e4fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_spacy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             raise ValueError(\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             )\n\u001b[1;32m    438\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 11986988 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "data_spacy = nlp_sp(data_clean)\n",
    "data_spacy[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced \n",
    "Let's look at custom tokenization. Sometimes Spacey won't tokenize as you would like but you can customize your own tokenizer to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[obj.text for obj in Alice_spacy.sents] # at the sentence level\n",
    "[token for token in Alice_spacy] # at the word level\n",
    "[(token, token.pos_) for token in Alice_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in Alice_spacy.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(token, token.ent_iob_) for token in Alice_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(token, token.is_stop) for token in Alice_spacy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a function that will convert a plain text into a dataframe all the necessary functions and build your NLP pipeline. Test your function on the following documents. \n",
    "\n",
    "\n",
    "Explore these documents. How many words are in each? What is the distribution of POS tags? How accurate is the tokenization and the pos tagging for the CS_interview and CS_novel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
